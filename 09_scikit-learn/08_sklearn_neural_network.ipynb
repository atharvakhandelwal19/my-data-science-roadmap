{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a64d619",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928d65f",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999f539",
   "metadata": {},
   "source": [
    "- It is a supervised learning algorithm.\n",
    "- MLP learns a non-linear function approximator for either classification or regression depending on the given dataset.\n",
    "- In sklearn, we implement MLP using:\n",
    "    - MLPClassifier for classification\n",
    "    - MLPClassifier for regression\n",
    "- MLPClassifier supports multi-class classification by applying Softmax as the output function.\n",
    "- It also supports supports multi-label classification in which a sample can belong to more than one class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac173ea1",
   "metadata": {},
   "source": [
    "## MLPClassifier\n",
    "\n",
    "- Step 1: Instantiate a MLP classifier estimator.\n",
    "- Step 2: Call fit method on MLP classifier object with training feature matrix and label vector as arguments.\n",
    "- Step 3: After fitting (training), the model can make predictions for new samples (X_test) using two methods:\n",
    "    - predict\n",
    "    - predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99859a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP_clf = MLPClassifier()\n",
    "X, y = 0\n",
    "MLP_clf.fit(X, y)\n",
    "\n",
    "MLP_clf.predict(X)\n",
    "MLP_clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f506d",
   "metadata": {},
   "source": [
    "### How to set the number of hidden layers?\n",
    "\n",
    "`hidden_layer_sizes` This parameter sets the number of layers and the number of neurons in each layer.\n",
    "- The length of tuple denotes the total number of hidden layers in the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f0fc2",
   "metadata": {},
   "source": [
    "To create a 3 hidden layer neural network with 15 neurons in first layer, 10 neurons in second layer and 5 neurons in third layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPClassifier(hidden_layer_sizes=(15,10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80893fac",
   "metadata": {},
   "source": [
    "### How to perform regularization in MLPClassifier?\n",
    "- The alpha parameter sets L2 penalty Regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94597d5f",
   "metadata": {},
   "source": [
    "### How to set the activation function for the hidden layers?\n",
    "\n",
    "- identity\n",
    "- logistic\n",
    "- tanh\n",
    "- relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0090e1",
   "metadata": {},
   "source": [
    "### How to perform weight optimization in MLPClassifier?\n",
    "\n",
    "- MLPClassifier optimizes the log-loss function using LBFGS or stochastic gradient descent\n",
    "- `Solver`:\n",
    "    - lbfgs\n",
    "    - sgd\n",
    "    - adam\n",
    "\n",
    "- If the solver is 'lbfgs', the classifier will not use minibatch.\n",
    "- Size of minibatches can be set to other stochastic optimizers: batch_size (int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952eb6a0",
   "metadata": {},
   "source": [
    "### How to view weight matrix coefficients of trained MLPClassifier?\n",
    "\n",
    "- coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ad643",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MLP_clf.coefs_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f0ede",
   "metadata": {},
   "source": [
    "## MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9744d0",
   "metadata": {},
   "source": [
    "- MLPRegressor trains using backpropagation with no activation function in the output layer.\n",
    "- Therefore, it uses the square error as the loss function, and the output is a set of continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_reg = MLPRegressor()\n",
    "mlp_reg.fit(X)\n",
    "mlp_reg.score(X,y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
